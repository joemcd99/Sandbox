{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Faceted B2B Name Matching\n",
        "\n",
        "This notebook converts the script into a step-by-step workflow.\n",
        "\n",
        "Each section isolates one key concept:\n",
        "1. Setup and dependencies\n",
        "2. Business-specific normalization rules\n",
        "3. Parsing and profile creation\n",
        "4. Scoring components\n",
        "5. Hard gates and penalties\n",
        "6. Ranking and one-to-one controls\n",
        "7. Labeled evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports used across the notebook.\n",
        "# pandas: tabular data operations\n",
        "# rapidfuzz: fast fuzzy string similarity\n",
        "# jellyfish/unidecode: optional phonetic + text normalization helpers\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import Dict, List, Set, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "try:\n",
        "    import jellyfish\n",
        "except Exception:\n",
        "    jellyfish = None\n",
        "\n",
        "try:\n",
        "    from unidecode import unidecode\n",
        "except Exception:\n",
        "    unidecode = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Business Normalization Dictionaries\n",
        "\n",
        "This section defines the rules used to normalize company names.\n",
        "\n",
        "Why this matters:\n",
        "- Legal suffixes (`Ltd`, `Limited`, `Inc`, `Co`, `LLC`, etc.) are often present in one source and absent in another.\n",
        "- Synonyms and abbreviations (`MFG` vs `Manufacturing`) should map to a shared canonical token.\n",
        "- Generic words (`Group`, `Holdings`) can create false positives if not controlled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Legal suffix tokens removed during normalization.\n",
        "# These usually do not distinguish one company from another.\n",
        "LEGAL_SUFFIXES = {\n",
        "    \"inc\", \"incorporated\", \"corp\", \"corporation\", \"co\", \"company\",\n",
        "    \"ltd\", \"limited\", \"llc\", \"l l c\", \"llp\", \"lp\", \"plc\",\n",
        "    \"gmbh\", \"sa\", \"ag\", \"bv\", \"pte\", \"pty\", \"sarl\",\n",
        "}\n",
        "\n",
        "# Extra words that are often non-unique in business names.\n",
        "BUSINESS_NOISE = {\n",
        "    \"the\", \"group\", \"holdings\", \"holding\", \"partners\", \"ventures\",\n",
        "    \"international\", \"global\",\n",
        "}\n",
        "\n",
        "# Token-level normalization aliases for common abbreviations.\n",
        "TOKEN_ALIASES = {\n",
        "    \"&\": \"and\",\n",
        "    \"intl\": \"international\",\n",
        "    \"int'l\": \"international\",\n",
        "    \"technologies\": \"technology\",\n",
        "    \"tech\": \"technology\",\n",
        "    \"svcs\": \"services\",\n",
        "    \"svc\": \"services\",\n",
        "    \"mfg\": \"manufacturing\",\n",
        "    \"manufacture\": \"manufacturing\",\n",
        "    \"mgmt\": \"management\",\n",
        "}\n",
        "\n",
        "# Aliases used when building anchor keys (last significant token).\n",
        "ANCHOR_ALIASES = {\n",
        "    \"oneil\": \"oneill\",\n",
        "    \"o'neil\": \"oneill\",\n",
        "    \"hlth\": \"health\",\n",
        "}\n",
        "\n",
        "# If anchor is very generic, score should be discounted.\n",
        "GENERIC_CORE_TOKENS = {\n",
        "    \"solutions\", \"systems\", \"services\", \"consulting\",\n",
        "    \"logistics\", \"trading\", \"management\", \"technology\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data Model + Parsing Helpers\n",
        "\n",
        "We build a `ParsedBusiness` profile once per name and reuse it.\n",
        "\n",
        "Why this helps:\n",
        "- Keeps matching logic fast and deterministic.\n",
        "- Separates raw text from normalized/computed features.\n",
        "- Produces reusable keys for blocking and strict gates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ParsedBusiness:\n",
        "    # Original input name from source system.\n",
        "    raw: str\n",
        "    # Canonical normalized string after cleanup and token mapping.\n",
        "    normalized: str\n",
        "    # Ordered normalized tokens.\n",
        "    tokens: List[str]\n",
        "    # Set form used for overlap checks.\n",
        "    token_set: Set[str]\n",
        "    # Last meaningful token (e.g., 'logistics', 'industrial').\n",
        "    anchor: str\n",
        "    # First meaningful token (used in blocking key).\n",
        "    first_token: str\n",
        "    # Acronym derived from normalized tokens.\n",
        "    acronym: str\n",
        "    # Optional phonetic fingerprints.\n",
        "    metaphone: str\n",
        "    soundex: str\n",
        "    # Blocking key to reduce candidate explosion.\n",
        "    block_key: str\n",
        "\n",
        "\n",
        "def _ascii_fold(value: str) -> str:\n",
        "    # Prefer unidecode if available, else standard Unicode decomposition.\n",
        "    if unidecode is not None:\n",
        "        return unidecode(value)\n",
        "    return unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "\n",
        "def _safe_metaphone(value: str) -> str:\n",
        "    if jellyfish is None or not value:\n",
        "        return \"\"\n",
        "    return jellyfish.metaphone(value)\n",
        "\n",
        "\n",
        "def _safe_soundex(value: str) -> str:\n",
        "    if jellyfish is None or not value:\n",
        "        return \"\"\n",
        "    return jellyfish.soundex(value)\n",
        "\n",
        "\n",
        "def _safe_jaro_winkler(a: str, b: str) -> float:\n",
        "    # Jaro-Winkler is often useful for near-typo business names.\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    if jellyfish is None:\n",
        "        return fuzz.WRatio(a, b) / 100.0\n",
        "    return float(jellyfish.jaro_winkler_similarity(a, b))\n",
        "\n",
        "\n",
        "def _normalize_token(token: str) -> str:\n",
        "    token = token.strip().lower()\n",
        "    if token in TOKEN_ALIASES:\n",
        "        token = TOKEN_ALIASES[token]\n",
        "    return ANCHOR_ALIASES.get(token, token)\n",
        "\n",
        "\n",
        "def _tokenize_business(value: str) -> List[str]:\n",
        "    # 1) normalize punctuation/symbols\n",
        "    cleaned = value.lower().replace(\"&\", \" and \")\n",
        "    cleaned = re.sub(r\"[^a-z0-9\\s']\", \" \", cleaned)\n",
        "\n",
        "    # 2) map tokens to canonical aliases\n",
        "    tokens = [_normalize_token(t.strip(\"'\")) for t in cleaned.split() if t.strip(\"'\")]\n",
        "\n",
        "    # 3) drop legal suffixes and generic noise\n",
        "    filtered: List[str] = []\n",
        "    for t in tokens:\n",
        "        if not t:\n",
        "            continue\n",
        "        if t in LEGAL_SUFFIXES:\n",
        "            continue\n",
        "        if t in BUSINESS_NOISE:\n",
        "            continue\n",
        "        filtered.append(t)\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def _normalize_anchor(token: str) -> str:\n",
        "    if not token:\n",
        "        return \"\"\n",
        "    return ANCHOR_ALIASES.get(token, token)\n",
        "\n",
        "\n",
        "def parse_and_normalize_name(raw_name: str) -> ParsedBusiness:\n",
        "    # Keep function name for compatibility with existing script usage.\n",
        "    raw_name = \"\" if raw_name is None else str(raw_name)\n",
        "    folded = _ascii_fold(raw_name)\n",
        "\n",
        "    tokens = _tokenize_business(folded)\n",
        "    normalized = \" \".join(tokens)\n",
        "\n",
        "    # Feature engineering for matching controls.\n",
        "    acronym = \"\".join(t[0] for t in tokens if t)\n",
        "    anchor = _normalize_anchor(tokens[-1]) if tokens else \"\"\n",
        "    first_token = tokens[0] if tokens else \"\"\n",
        "\n",
        "    # Blocking key: first initial + anchor phonetic fingerprint.\n",
        "    anchor_soundex = _safe_soundex(anchor)\n",
        "    first_initial = first_token[0] if first_token else \"\"\n",
        "    block_key = f\"{first_initial}:{anchor_soundex or anchor}\"\n",
        "\n",
        "    return ParsedBusiness(\n",
        "        raw=raw_name,\n",
        "        normalized=normalized,\n",
        "        tokens=tokens,\n",
        "        token_set=set(tokens),\n",
        "        anchor=anchor,\n",
        "        first_token=first_token,\n",
        "        acronym=acronym,\n",
        "        metaphone=_safe_metaphone(normalized.replace(\" \", \"\")),\n",
        "        soundex=_safe_soundex(normalized.replace(\" \", \"\")),\n",
        "        block_key=block_key,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Match Configuration + Scoring Components\n",
        "\n",
        "We combine multiple signals into one score:\n",
        "- exact normalized match\n",
        "- token sort ratio\n",
        "- token set ratio\n",
        "- Jaro-Winkler similarity\n",
        "- phonetic agreement\n",
        "- acronym similarity\n",
        "\n",
        "Weighted scoring avoids over-relying on one fuzzy metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class MatchConfig:\n",
        "    # Decision thresholds for output bands.\n",
        "    auto_threshold: float = 92.0\n",
        "    review_threshold: float = 84.0\n",
        "    reject_threshold: float = 74.0\n",
        "\n",
        "    # Minimum component quality gates.\n",
        "    min_token_set: float = 88.0\n",
        "    min_jaro_winkler: float = 86.0\n",
        "\n",
        "    # Gate switches.\n",
        "    require_anchor_match: bool = True\n",
        "    require_token_overlap: bool = True\n",
        "    min_shared_tokens: int = 1\n",
        "\n",
        "    # Difference required between top and runner-up candidates.\n",
        "    top_margin_required: float = 4.0\n",
        "\n",
        "    # Signal weights for final score.\n",
        "    weights: Dict[str, float] | None = None\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if self.weights is None:\n",
        "            self.weights = {\n",
        "                \"exact\": 0.30,\n",
        "                \"token_sort\": 0.20,\n",
        "                \"token_set\": 0.25,\n",
        "                \"jaro_winkler\": 0.10,\n",
        "                \"phonetic\": 0.10,\n",
        "                \"acronym\": 0.05,\n",
        "            }\n",
        "\n",
        "\n",
        "def _anchors_match(left: ParsedBusiness, right: ParsedBusiness) -> bool:\n",
        "    return bool(left.anchor and right.anchor and left.anchor == right.anchor)\n",
        "\n",
        "\n",
        "def _shared_token_count(left: ParsedBusiness, right: ParsedBusiness) -> int:\n",
        "    return len(left.token_set.intersection(right.token_set))\n",
        "\n",
        "\n",
        "def _component_scores(left: ParsedBusiness, right: ParsedBusiness) -> Dict[str, float]:\n",
        "    exact = 100.0 if left.normalized and left.normalized == right.normalized else 0.0\n",
        "    token_sort = float(fuzz.token_sort_ratio(left.normalized, right.normalized))\n",
        "    token_set = float(fuzz.token_set_ratio(left.normalized, right.normalized))\n",
        "    jaro_winkler = _safe_jaro_winkler(left.normalized, right.normalized) * 100.0\n",
        "\n",
        "    phonetic = 0.0\n",
        "    if left.metaphone and right.metaphone and left.metaphone == right.metaphone:\n",
        "        phonetic += 50.0\n",
        "    if left.soundex and right.soundex and left.soundex == right.soundex:\n",
        "        phonetic += 50.0\n",
        "\n",
        "    acronym = 0.0\n",
        "    if left.acronym and right.acronym:\n",
        "        acronym = float(fuzz.ratio(left.acronym, right.acronym))\n",
        "\n",
        "    return {\n",
        "        \"exact\": exact,\n",
        "        \"token_sort\": token_sort,\n",
        "        \"token_set\": token_set,\n",
        "        \"jaro_winkler\": jaro_winkler,\n",
        "        \"phonetic\": phonetic,\n",
        "        \"acronym\": acronym,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Controls: Gates, Penalties, and Decision Bands\n",
        "\n",
        "This is where false positives are reduced.\n",
        "\n",
        "Controls applied:\n",
        "- Hard gate on anchor alignment (business equivalent of strict key token matching)\n",
        "- Minimum token overlap and component minimums\n",
        "- Penalties for underspecified names or generic anchors\n",
        "- Decision bands (`auto_match`, `manual_review`, `weak_match`, `reject`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _penalty_points(left: ParsedBusiness, right: ParsedBusiness) -> float:\n",
        "    penalty = 0.0\n",
        "\n",
        "    # Very short entity names are ambiguous.\n",
        "    if len(left.tokens) <= 1 or len(right.tokens) <= 1:\n",
        "        penalty += 15.0\n",
        "\n",
        "    # Large token-count mismatch increases risk.\n",
        "    token_gap = abs(len(left.tokens) - len(right.tokens))\n",
        "    if token_gap >= 2:\n",
        "        penalty += 8.0\n",
        "    elif token_gap == 1:\n",
        "        penalty += 3.0\n",
        "\n",
        "    # Generic anchors require extra caution.\n",
        "    if left.anchor and right.anchor and left.anchor == right.anchor:\n",
        "        if left.anchor in GENERIC_CORE_TOKENS:\n",
        "            penalty += 7.0\n",
        "\n",
        "    # If overlap is only the generic anchor, penalize.\n",
        "    if _shared_token_count(left, right) == 1 and left.anchor == right.anchor:\n",
        "        penalty += 5.0\n",
        "\n",
        "    return penalty\n",
        "\n",
        "\n",
        "def _passes_hard_gates(\n",
        "    left: ParsedBusiness,\n",
        "    right: ParsedBusiness,\n",
        "    components: Dict[str, float],\n",
        "    config: MatchConfig,\n",
        ") -> Tuple[bool, str]:\n",
        "    if config.require_anchor_match and not _anchors_match(left, right):\n",
        "        return False, \"failed_anchor_gate\"\n",
        "\n",
        "    if config.require_token_overlap and _shared_token_count(left, right) < config.min_shared_tokens:\n",
        "        return False, \"failed_token_overlap_gate\"\n",
        "\n",
        "    if components[\"token_set\"] < config.min_token_set:\n",
        "        return False, \"failed_token_set_minimum\"\n",
        "\n",
        "    if components[\"jaro_winkler\"] < config.min_jaro_winkler:\n",
        "        return False, \"failed_jaro_minimum\"\n",
        "\n",
        "    return True, \"passed\"\n",
        "\n",
        "\n",
        "def _decision_band(score: float, config: MatchConfig) -> str:\n",
        "    if score >= config.auto_threshold:\n",
        "        return \"auto_match\"\n",
        "    if score >= config.review_threshold:\n",
        "        return \"manual_review\"\n",
        "    if score >= config.reject_threshold:\n",
        "        return \"weak_match\"\n",
        "    return \"reject\"\n",
        "\n",
        "\n",
        "def _blocked_candidate_indices(left_profile: ParsedBusiness, right_profiles: Dict[int, ParsedBusiness]) -> List[int]:\n",
        "    # First pass: strict block key.\n",
        "    in_block = [idx for idx, p in right_profiles.items() if p.block_key == left_profile.block_key]\n",
        "    if in_block:\n",
        "        return in_block\n",
        "\n",
        "    # Fallback: same anchor.\n",
        "    fallback = [idx for idx, p in right_profiles.items() if _anchors_match(left_profile, p)]\n",
        "    if fallback:\n",
        "        return fallback\n",
        "\n",
        "    # Final fallback preserves recall for edge cases.\n",
        "    return list(right_profiles.keys())\n",
        "\n",
        "\n",
        "def score_name_pair(\n",
        "    left: ParsedBusiness,\n",
        "    right: ParsedBusiness,\n",
        "    config: MatchConfig,\n",
        ") -> Tuple[float, Dict[str, float], str]:\n",
        "    components = _component_scores(left, right)\n",
        "\n",
        "    passed, gate_reason = _passes_hard_gates(left, right, components, config)\n",
        "    if not passed:\n",
        "        return 0.0, components, gate_reason\n",
        "\n",
        "    weighted_sum = sum(components[k] * config.weights[k] for k in config.weights)\n",
        "    weighted_score = weighted_sum / sum(config.weights.values())\n",
        "\n",
        "    final = max(0.0, weighted_score - _penalty_points(left, right))\n",
        "    return round(final, 2), components, gate_reason\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sample Data and Labels\n",
        "\n",
        "This sample intentionally includes:\n",
        "- legal suffix variations (`Ltd`, `Limited`, `Co`, `Company`, `Inc`)\n",
        "- abbreviation variations (`MFG` vs `Manufacturing`)\n",
        "- distractor records to test false-positive resistance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_sample_dataframes() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    left = pd.DataFrame(\n",
        "        {\n",
        "            \"record_id\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "            \"name\": [\n",
        "                \"North Star Logistics Ltd.\",\n",
        "                \"Acme Industrial Co.\",\n",
        "                \"Blue River Technologies Inc\",\n",
        "                \"Summit Health Services LLC\",\n",
        "                \"Redwood Manufacturing Limited\",\n",
        "                \"Pioneer Energy Holdings\",\n",
        "                \"Global Trade Partners Inc.\",\n",
        "                \"Urban Data Systems Co\",\n",
        "            ],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    right = pd.DataFrame(\n",
        "        {\n",
        "            \"customer_id\": [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
        "            \"full_name\": [\n",
        "                \"North Star Logistics Limited\",\n",
        "                \"ACME Industrial Company\",\n",
        "                \"Blue River Technology Incorporated\",\n",
        "                \"Summit Health Service\",\n",
        "                \"Redwood MFG Ltd\",\n",
        "                \"Pioneer Energy\",\n",
        "                \"Global Trading Partners\",\n",
        "                \"Urban Data System Inc\",\n",
        "                \"Northern Star Logistics Co\",\n",
        "                \"Acme Logistics Inc\",\n",
        "            ],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return left, right\n",
        "\n",
        "\n",
        "def build_sample_labels() -> pd.DataFrame:\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"left_name\": [\n",
        "                \"North Star Logistics Ltd.\",\n",
        "                \"Acme Industrial Co.\",\n",
        "                \"Blue River Technologies Inc\",\n",
        "                \"Pioneer Energy Holdings\",\n",
        "                \"Acme Industrial Co.\",\n",
        "                \"Urban Data Systems Co\",\n",
        "            ],\n",
        "            \"right_name\": [\n",
        "                \"North Star Logistics Limited\",\n",
        "                \"ACME Industrial Company\",\n",
        "                \"Blue River Technology Incorporated\",\n",
        "                \"Pioneer Energy\",\n",
        "                \"Acme Logistics Inc\",\n",
        "                \"Global Trading Partners\",\n",
        "            ],\n",
        "            \"is_true_match\": [1, 1, 1, 1, 0, 0],\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Ranking Pipeline and One-to-One Assignment\n",
        "\n",
        "The ranking process does the following:\n",
        "1. Parse left and right names into profiles.\n",
        "2. Use blocking to limit candidate comparisons.\n",
        "3. Score each candidate with gates + penalties.\n",
        "4. Apply decision bands.\n",
        "5. Enforce one-to-one preference and top-margin checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rank_matches(\n",
        "    df_left: pd.DataFrame,\n",
        "    df_right: pd.DataFrame,\n",
        "    left_col: str = \"name\",\n",
        "    right_col: str = \"full_name\",\n",
        "    top_n: int = 3,\n",
        "    config: MatchConfig | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    config = config or MatchConfig()\n",
        "\n",
        "    left_profiles = {idx: parse_and_normalize_name(v) for idx, v in df_left[left_col].fillna(\"\").items()}\n",
        "    right_profiles = {idx: parse_and_normalize_name(v) for idx, v in df_right[right_col].fillna(\"\").items()}\n",
        "\n",
        "    rows: List[Dict[str, object]] = []\n",
        "\n",
        "    for left_idx, left_name in left_profiles.items():\n",
        "        per_left: List[Dict[str, object]] = []\n",
        "        candidate_indices = _blocked_candidate_indices(left_name, right_profiles)\n",
        "\n",
        "        for right_idx in candidate_indices:\n",
        "            right_name = right_profiles[right_idx]\n",
        "            score, details, gate_reason = score_name_pair(left_name, right_name, config)\n",
        "            band = _decision_band(score, config)\n",
        "            if band == \"reject\":\n",
        "                continue\n",
        "\n",
        "            per_left.append(\n",
        "                {\n",
        "                    \"left_index\": left_idx,\n",
        "                    \"left_name\": left_name.raw,\n",
        "                    \"left_normalized\": left_name.normalized,\n",
        "                    \"left_anchor\": left_name.anchor,\n",
        "                    \"right_index\": right_idx,\n",
        "                    \"right_name\": right_name.raw,\n",
        "                    \"right_normalized\": right_name.normalized,\n",
        "                    \"right_anchor\": right_name.anchor,\n",
        "                    \"match_score\": score,\n",
        "                    \"decision_band\": band,\n",
        "                    \"gate_reason\": gate_reason,\n",
        "                    \"shared_tokens\": _shared_token_count(left_name, right_name),\n",
        "                    \"penalty_points\": round(_penalty_points(left_name, right_name), 2),\n",
        "                    **{f\"component_{k}\": round(v, 2) for k, v in details.items()},\n",
        "                }\n",
        "            )\n",
        "\n",
        "        per_left.sort(key=lambda r: r[\"match_score\"], reverse=True)\n",
        "\n",
        "        # If top two candidates are too close, force manual review.\n",
        "        if len(per_left) >= 2:\n",
        "            top_delta = per_left[0][\"match_score\"] - per_left[1][\"match_score\"]\n",
        "            per_left[0][\"top_margin\"] = round(top_delta, 2)\n",
        "            if top_delta < config.top_margin_required and per_left[0][\"decision_band\"] == \"auto_match\":\n",
        "                per_left[0][\"decision_band\"] = \"manual_review\"\n",
        "                per_left[0][\"gate_reason\"] = \"tight_runner_up_margin\"\n",
        "        elif len(per_left) == 1:\n",
        "            per_left[0][\"top_margin\"] = 999.0\n",
        "\n",
        "        rows.extend(per_left[:top_n])\n",
        "\n",
        "    result = pd.DataFrame(rows)\n",
        "    if result.empty:\n",
        "        return result\n",
        "\n",
        "    # One-to-one preference: keep best left candidate per right record.\n",
        "    result = result.sort_values([\"right_index\", \"match_score\"], ascending=[True, False])\n",
        "    dedup_rows = []\n",
        "    for _, group in result.groupby(\"right_index\", as_index=False):\n",
        "        top = group.iloc[0].copy()\n",
        "        if len(group) >= 2:\n",
        "            margin = float(group.iloc[0][\"match_score\"] - group.iloc[1][\"match_score\"])\n",
        "            if margin < config.top_margin_required and top[\"decision_band\"] == \"auto_match\":\n",
        "                top[\"decision_band\"] = \"manual_review\"\n",
        "                top[\"gate_reason\"] = \"right_side_competition\"\n",
        "        dedup_rows.append(top)\n",
        "\n",
        "    final = pd.DataFrame(dedup_rows)\n",
        "    final = final.sort_values([\"left_index\", \"match_score\"], ascending=[True, False])\n",
        "    return final.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Labeled Evaluation and Demo Run\n",
        "\n",
        "This final section gives quick metrics to validate tuning choices.\n",
        "\n",
        "Key metric focus for production tuning:\n",
        "- Precision (keep this high to avoid false positives)\n",
        "- False positive rate\n",
        "- Recall (track tradeoff when tightening controls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_labels(\n",
        "    labels_df: pd.DataFrame,\n",
        "    left_col: str = \"left_name\",\n",
        "    right_col: str = \"right_name\",\n",
        "    label_col: str = \"is_true_match\",\n",
        "    config: MatchConfig | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    config = config or MatchConfig()\n",
        "\n",
        "    y_true: List[int] = []\n",
        "    y_pred: List[int] = []\n",
        "\n",
        "    for _, row in labels_df.iterrows():\n",
        "        left = parse_and_normalize_name(row[left_col])\n",
        "        right = parse_and_normalize_name(row[right_col])\n",
        "        score, _, _ = score_name_pair(left, right, config)\n",
        "        pred = 1 if score >= config.auto_threshold else 0\n",
        "        y_true.append(int(row[label_col]))\n",
        "        y_pred.append(pred)\n",
        "\n",
        "    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
        "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
        "    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n",
        "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"metric\": [\"precision\", \"recall\", \"false_positive_rate\", \"tp\", \"fp\", \"fn\", \"tn\"],\n",
        "            \"value\": [round(precision, 4), round(recall, 4), round(fpr, 4), tp, fp, fn, tn],\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# Run end-to-end demo.\n",
        "left_df, right_df = build_sample_dataframes()\n",
        "config = MatchConfig()\n",
        "matches_df = rank_matches(left_df, right_df, left_col=\"name\", right_col=\"full_name\", top_n=3, config=config)\n",
        "metrics_df = evaluate_with_labels(build_sample_labels(), config=config)\n",
        "\n",
        "print(\"Sample Left DataFrame\")\n",
        "display(left_df)\n",
        "print(\"Sample Right DataFrame\")\n",
        "display(right_df)\n",
        "print(\"Ranked Matches\")\n",
        "display(matches_df)\n",
        "print(\"Labeled Evaluation\")\n",
        "display(metrics_df)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}